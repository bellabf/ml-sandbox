{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Overfitting with Weight Regularization\n",
    "One strategy to combat overfitting neural networks is by penalizing the parameters\n",
    "(i.e., weights) of the neural network such that they are driven to be small values,\n",
    "creating a simpler model less prone to overfit. This method is called weight regulari‐\n",
    "zation or weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.1, random_state=1)\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4532115161418915 \tTest Accuracy: 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16,16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x\n",
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "# Train neural network\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# Evaluate neural network\n",
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "    test_accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Overfitting with Early Stopping\n",
    "one of the most common and very effective methods to counter overfitting is to monitor the training process and stop training when the test error starts to increase. This strategy is called early stopping.\n",
    "\n",
    "here we use the popular library lightning (known as PyTorch Lightning) to use an out-of-the-box one. PyTorch Lightning is a high-level library for PyTorch that provides a lot of useful features. In our solution, we included PyTorch Lightning’s\n",
    "EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3) to define that we wanted to monitor the test (validation) loss at each epoch, and if the test loss has not improved after three epochs (the default), training is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.1, random_state=1)\n",
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16,16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningNetwork(pl.LightningModule):\n",
    "    def __init__(self, network):\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.metric = nn.functional.binary_cross_entropy\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        data, target = batch\n",
    "        output = self.network(data)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/isabella/Documents/notes_ml/lightning_logs\n",
      "2023-12-27 23:53:15.558546: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-27 23:53:15.602277: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-27 23:53:15.837255: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-27 23:53:15.837492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-27 23:53:15.869034: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 23:53:15.975274: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-27 23:53:15.977627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 23:53:17.442050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "  | Name      | Type            | Params\n",
      "----------------------------------------------\n",
      "0 | network   | SimpleNeuralNet | 465   \n",
      "1 | criterion | BCELoss         | 0     \n",
      "----------------------------------------------\n",
      "465       Trainable params\n",
      "0         Non-trainable params\n",
      "465       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "/home/isabella/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/isabella/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af930f7ff96440b96d9403c668e0726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "# Initialize neural network\n",
    "network = LightningNetwork(SimpleNeuralNet())\n",
    "# Train network\n",
    "trainer = pl.Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "patience=3)], max_epochs=1000)\n",
    "trainer.fit(model=network, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Overfitting with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a fairly common method for regularizing smaller neural networks. In\n",
    "dropout, every time a batch of observations is created for training, a proportion of\n",
    "the units in one or more layers is multiplied by zero (i.e., dropped). In this setting,\n",
    "every batch is trained on the same network (e.g., the same parameters), but each\n",
    "batch is confronted by a slightly different version of that network’s architecture. they learn to be robust to disruptions\n",
    "(i.e., noise) in the other hidden units, and this prevents the network from simply\n",
    "memorizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "features, target = make_classification(n_classes=2, n_features=10,\n",
    "n_samples=1000)\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# Convert data to PyTorch tensors\n",
    "x_train = torch.from_numpy(features_train).float()\n",
    "y_train = torch.from_numpy(target_train).float().view(-1, 1)\n",
    "x_test = torch.from_numpy(features_test).float()\n",
    "y_test = torch.from_numpy(target_test).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network using `Sequential`\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "        torch.nn.Linear(10, 16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16,16),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(16, 1),\n",
    "        torch.nn.Dropout(0.1), # Drop 10% of neurons\n",
    "        torch.nn.Sigmoid(),\n",
    "    )\n",
    "    def forward(self, x):\n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.20735645294189453\n",
      "Epoch: 2 \tLoss: 0.20834866166114807\n",
      "Epoch: 3 \tLoss: 0.25975286960601807\n"
     ]
    }
   ],
   "source": [
    "# Initialize neural network\n",
    "network = SimpleNeuralNet()\n",
    "# Define loss function, optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = RMSprop(network.parameters())\n",
    "# Define data loader\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "network = torch.compile(network)\n",
    "# Train neural network\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch:\", epoch+1, \"\\tLoss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1384032666683197 \tTest Accuracy: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = network(x_test)\n",
    "    test_loss = criterion(output, y_test)\n",
    "    test_accuracy = (output.round() == y_test).float().mean()\n",
    "    print(\"Test Loss:\", test_loss.item(), \"\\tTest Accuracy:\",\n",
    "    test_accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
