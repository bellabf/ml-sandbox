{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_row = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_col = torch.tensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from numpy to torchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row=np.array([1, 0, 1])\n",
    "tensor_row=torch.from_numpy(vector_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create sparse tensors   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=np.zeros((10, 10))\n",
    "temp[0:5, 1:3] = 1\n",
    "\n",
    "tensor = torch.from_numpy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 1, 2, 2, 3, 3, 4, 4],\n",
       "                       [1, 2, 1, 2, 1, 2, 1, 2, 1, 2]]),\n",
       "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       "       size=(10, 10), nnz=10, dtype=torch.float64, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_tensor=tensor.to_sparse()\n",
    "sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0:3, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterCPU.cpp:31188 [kernel]\nMeta: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterMeta.cpp:26829 [kernel]\nQuantizedCPU: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nBackendSelect: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21905 [kernel]\nNamed: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\nAutogradOther: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:714 [kernel]\nFuncTorchVmapMode: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sparse_tensor[\u001b[39m0\u001b[39;49m:\u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m:\u001b[39m2\u001b[39;49m]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterCPU.cpp:31188 [kernel]\nMeta: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterMeta.cpp:26829 [kernel]\nQuantizedCPU: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nBackendSelect: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterFunctionalization_0.cpp:21905 [kernel]\nNamed: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/native/NegateFallback.cpp:23 [kernel]\nZeroTensor: registered at /croot/pytorch-select_1700158693612/work/build/aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]\nAutogradOther: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradCUDA: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHIP: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXLA: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMPS: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradIPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradXPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradHPU: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradVE: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradLazy: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMTIA: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse1: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse2: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradPrivateUse3: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradMeta: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nAutogradNestedTensor: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/VariableType_0.cpp:16790 [autograd kernel]\nTracer: registered at /croot/pytorch-select_1700158693612/work/torch/csrc/autograd/generated/TraceType_0.cpp:16725 [kernel]\nAutocastCPU: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/autocast_mode.cpp:382 [backend fallback]\nAutocastCUDA: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/autocast_mode.cpp:249 [backend fallback]\nFuncTorchBatched: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:714 [kernel]\nFuncTorchVmapMode: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at /croot/pytorch-select_1700158693612/work/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "sparse_tensor[0:3, 1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliced tensor is still a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor[:, ::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n",
      "\u001b[0;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "tensor[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.flip(dims=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing tensors      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.strided"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor operations   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.mT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 0., 0., 0., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_3d= np.ones((5, 5, 5))\n",
    "small_3d[0:2, 3:4, 1:4]=0\n",
    "tensor_3d=torch.from_numpy(small_3d)\n",
    "tensor_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_149230/1924630843.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /croot/pytorch-select_1700158693612/work/aten/src/ATen/native/TensorShape.cpp:3614.)\n",
      "  tensor_3d.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 0., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3d.mT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1=torch.tensor([1, 3, 5])\n",
    "tensor_2=torch.tensor([1, 2, 4])\n",
    "tensor_1.dot(tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  6, 20])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1 * tensor_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neuron: takes an input an multiple it by a parameter or weight, sums it up along some bias then it is feed into an activation function\n",
    "- Input layer: each unit is an observation for a feature\n",
    "- Hidden layers: layers that transform the feature values from the input layer into something that is usefull for the targeted class\n",
    "- Output layer: transform the intermediate inputs into values useful for the task at hand\n",
    "- Forward propragation\n",
    "- Back propragation\n",
    "Weights are tipically initialized either from random sampling from a gaussian or normal uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN do poorly when they are not scale. Scale your damn data!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd: a way to automatically compute and store the gradients used to optimize the parameters of the netowrks after going foward and backwards propragation\n",
    "\n",
    "Autograd use a directed acyclic graph to keep track of the data and computations used in it. This means you cannot go pytorch->numpy and numpy<-pytorch easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.tensor([2.0, 4.0, 6.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating foward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sum= t.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating backward propragation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sum.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch -> numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 6.], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/!\\ this will stop numpy from automatically calculating the gradient /!\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to scale data that you have `requires_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_scaled_tensor=torch.tensor([[1000.2, 9.32],\n",
    "                                [-9000.1, -9067.675],\n",
    "                                [3948.3, -1981.087],\n",
    "                                [2313.4, -112.674]],\n",
    "                                requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1000.2000,     9.3200],\n",
       "        [-9000.0996, -9067.6748],\n",
       "        [ 3948.3000, -1981.0870],\n",
       "        [ 2313.3999,  -112.6740]], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_scaled_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=non_scaled_tensor.mean(0, keepdim=True)\n",
    "std= non_scaled_tensor.std(0, unbiased=False, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -434.5499, -2788.0288]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5054.3979, 3710.3838]], grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_tensor= (non_scaled_tensor - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2839,  0.7539],\n",
       "        [-1.6947, -1.6925],\n",
       "        [ 0.8671,  0.2175],\n",
       "        [ 0.5437,  0.7210]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A small example with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(1, 10, requires_grad=True)\n",
    "prev_h=torch.rand(1, 20, requires_grad=True)\n",
    "W_h= torch.rand(20, 20, requires_grad=True)\n",
    "W_x=torch.rand(20,10, requires_grad=True)\n",
    "i2h=torch.mm(W_x, x.t())\n",
    "h2h=torch.mm(W_h, prev_h.t())\n",
    "next_h= i2h + h2h\n",
    "next_h= next_h.tanh()\n",
    "loss= next_h.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.9191e-08, 1.1251e-07, 2.0231e-07, 5.5796e-08, 6.1820e-08, 1.7643e-07,\n",
      "         2.7142e-08, 1.5448e-07, 6.0156e-08, 1.6152e-07],\n",
      "        [2.9757e-07, 3.3753e-07, 6.0694e-07, 1.6739e-07, 1.8546e-07, 5.2930e-07,\n",
      "         8.1427e-08, 4.6345e-07, 1.8047e-07, 4.8457e-07],\n",
      "        [4.9595e-08, 5.6255e-08, 1.0116e-07, 2.7898e-08, 3.0910e-08, 8.8216e-08,\n",
      "         1.3571e-08, 7.7241e-08, 3.0078e-08, 8.0763e-08],\n",
      "        [9.9191e-08, 1.1251e-07, 2.0231e-07, 5.5796e-08, 6.1820e-08, 1.7643e-07,\n",
      "         2.7142e-08, 1.5448e-07, 6.0156e-08, 1.6152e-07],\n",
      "        [7.4393e-07, 8.4382e-07, 1.5174e-06, 4.1847e-07, 4.6365e-07, 1.3232e-06,\n",
      "         2.0357e-07, 1.1586e-06, 4.5117e-07, 1.2114e-06],\n",
      "        [2.4798e-07, 2.8127e-07, 5.0578e-07, 1.3949e-07, 1.5455e-07, 4.4108e-07,\n",
      "         6.7856e-08, 3.8620e-07, 1.5039e-07, 4.0381e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.9595e-08, 5.6255e-08, 1.0116e-07, 2.7898e-08, 3.0910e-08, 8.8216e-08,\n",
      "         1.3571e-08, 7.7241e-08, 3.0078e-08, 8.0763e-08],\n",
      "        [9.9191e-08, 1.1251e-07, 2.0231e-07, 5.5796e-08, 6.1820e-08, 1.7643e-07,\n",
      "         2.7142e-08, 1.5448e-07, 6.0156e-08, 1.6152e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.9595e-08, 5.6255e-08, 1.0116e-07, 2.7898e-08, 3.0910e-08, 8.8216e-08,\n",
      "         1.3571e-08, 7.7241e-08, 3.0078e-08, 8.0763e-08],\n",
      "        [9.9191e-08, 1.1251e-07, 2.0231e-07, 5.5796e-08, 6.1820e-08, 1.7643e-07,\n",
      "         2.7142e-08, 1.5448e-07, 6.0156e-08, 1.6152e-07],\n",
      "        [6.9433e-07, 7.8757e-07, 1.4162e-06, 3.9057e-07, 4.3274e-07, 1.2350e-06,\n",
      "         1.9000e-07, 1.0814e-06, 4.2109e-07, 1.1307e-06],\n",
      "        [2.4798e-07, 2.8127e-07, 5.0578e-07, 1.3949e-07, 1.5455e-07, 4.4108e-07,\n",
      "         6.7856e-08, 3.8620e-07, 1.5039e-07, 4.0381e-07],\n",
      "        [4.9595e-08, 5.6255e-08, 1.0116e-07, 2.7898e-08, 3.0910e-08, 8.8216e-08,\n",
      "         1.3571e-08, 7.7241e-08, 3.0078e-08, 8.0763e-08],\n",
      "        [5.4555e-07, 6.1880e-07, 1.1127e-06, 3.0688e-07, 3.4001e-07, 9.7038e-07,\n",
      "         1.4928e-07, 8.4965e-07, 3.3086e-07, 8.8839e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.9595e-08, 5.6255e-08, 1.0116e-07, 2.7898e-08, 3.0910e-08, 8.8216e-08,\n",
      "         1.3571e-08, 7.7241e-08, 3.0078e-08, 8.0763e-08]]) tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.8070e-07, 2.0759e-07, 2.1000e-07, 1.6475e-08, 1.4793e-07, 8.1411e-08,\n",
      "         1.8918e-07, 4.5645e-08, 1.4773e-07, 2.3788e-07, 2.2415e-07, 8.0596e-08,\n",
      "         2.1206e-07, 1.2608e-07, 1.2908e-07, 1.5806e-07, 3.5950e-08, 1.2368e-08,\n",
      "         1.7547e-07, 2.2664e-07],\n",
      "        [5.4211e-07, 6.2277e-07, 6.3000e-07, 4.9424e-08, 4.4378e-07, 2.4423e-07,\n",
      "         5.6753e-07, 1.3694e-07, 4.4319e-07, 7.1363e-07, 6.7244e-07, 2.4179e-07,\n",
      "         6.3617e-07, 3.7824e-07, 3.8724e-07, 4.7417e-07, 1.0785e-07, 3.7104e-08,\n",
      "         5.2642e-07, 6.7993e-07],\n",
      "        [9.0351e-08, 1.0379e-07, 1.0500e-07, 8.2373e-09, 7.3963e-08, 4.0706e-08,\n",
      "         9.4588e-08, 2.2823e-08, 7.3865e-08, 1.1894e-07, 1.1207e-07, 4.0298e-08,\n",
      "         1.0603e-07, 6.3041e-08, 6.4539e-08, 7.9028e-08, 1.7975e-08, 6.1840e-09,\n",
      "         8.7737e-08, 1.1332e-07],\n",
      "        [1.8070e-07, 2.0759e-07, 2.1000e-07, 1.6475e-08, 1.4793e-07, 8.1411e-08,\n",
      "         1.8918e-07, 4.5645e-08, 1.4773e-07, 2.3788e-07, 2.2415e-07, 8.0596e-08,\n",
      "         2.1206e-07, 1.2608e-07, 1.2908e-07, 1.5806e-07, 3.5950e-08, 1.2368e-08,\n",
      "         1.7547e-07, 2.2664e-07],\n",
      "        [1.3553e-06, 1.5569e-06, 1.5750e-06, 1.2356e-07, 1.1094e-06, 6.1058e-07,\n",
      "         1.4188e-06, 3.4234e-07, 1.1080e-06, 1.7841e-06, 1.6811e-06, 6.0447e-07,\n",
      "         1.5904e-06, 9.4561e-07, 9.6809e-07, 1.1854e-06, 2.6962e-07, 9.2760e-08,\n",
      "         1.3161e-06, 1.6998e-06],\n",
      "        [4.5175e-07, 5.1897e-07, 5.2500e-07, 4.1186e-08, 3.6981e-07, 2.0353e-07,\n",
      "         4.7294e-07, 1.1411e-07, 3.6932e-07, 5.9469e-07, 5.6037e-07, 2.0149e-07,\n",
      "         5.3014e-07, 3.1520e-07, 3.2270e-07, 3.9514e-07, 8.9875e-08, 3.0920e-08,\n",
      "         4.3869e-07, 5.6661e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.0351e-08, 1.0379e-07, 1.0500e-07, 8.2373e-09, 7.3963e-08, 4.0706e-08,\n",
      "         9.4588e-08, 2.2823e-08, 7.3865e-08, 1.1894e-07, 1.1207e-07, 4.0298e-08,\n",
      "         1.0603e-07, 6.3041e-08, 6.4539e-08, 7.9028e-08, 1.7975e-08, 6.1840e-09,\n",
      "         8.7737e-08, 1.1332e-07],\n",
      "        [1.8070e-07, 2.0759e-07, 2.1000e-07, 1.6475e-08, 1.4793e-07, 8.1411e-08,\n",
      "         1.8918e-07, 4.5645e-08, 1.4773e-07, 2.3788e-07, 2.2415e-07, 8.0596e-08,\n",
      "         2.1206e-07, 1.2608e-07, 1.2908e-07, 1.5806e-07, 3.5950e-08, 1.2368e-08,\n",
      "         1.7547e-07, 2.2664e-07],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.0351e-08, 1.0379e-07, 1.0500e-07, 8.2373e-09, 7.3963e-08, 4.0706e-08,\n",
      "         9.4588e-08, 2.2823e-08, 7.3865e-08, 1.1894e-07, 1.1207e-07, 4.0298e-08,\n",
      "         1.0603e-07, 6.3041e-08, 6.4539e-08, 7.9028e-08, 1.7975e-08, 6.1840e-09,\n",
      "         8.7737e-08, 1.1332e-07],\n",
      "        [1.8070e-07, 2.0759e-07, 2.1000e-07, 1.6475e-08, 1.4793e-07, 8.1411e-08,\n",
      "         1.8918e-07, 4.5645e-08, 1.4773e-07, 2.3788e-07, 2.2415e-07, 8.0596e-08,\n",
      "         2.1206e-07, 1.2608e-07, 1.2908e-07, 1.5806e-07, 3.5950e-08, 1.2368e-08,\n",
      "         1.7547e-07, 2.2664e-07],\n",
      "        [1.2649e-06, 1.4531e-06, 1.4700e-06, 1.1532e-07, 1.0355e-06, 5.6988e-07,\n",
      "         1.3242e-06, 3.1952e-07, 1.0341e-06, 1.6651e-06, 1.5690e-06, 5.6417e-07,\n",
      "         1.4844e-06, 8.8257e-07, 9.0355e-07, 1.1064e-06, 2.5165e-07, 8.6576e-08,\n",
      "         1.2283e-06, 1.5865e-06],\n",
      "        [4.5175e-07, 5.1897e-07, 5.2500e-07, 4.1186e-08, 3.6981e-07, 2.0353e-07,\n",
      "         4.7294e-07, 1.1411e-07, 3.6932e-07, 5.9469e-07, 5.6037e-07, 2.0149e-07,\n",
      "         5.3014e-07, 3.1520e-07, 3.2270e-07, 3.9514e-07, 8.9875e-08, 3.0920e-08,\n",
      "         4.3869e-07, 5.6661e-07],\n",
      "        [9.0351e-08, 1.0379e-07, 1.0500e-07, 8.2373e-09, 7.3963e-08, 4.0706e-08,\n",
      "         9.4588e-08, 2.2823e-08, 7.3865e-08, 1.1894e-07, 1.1207e-07, 4.0298e-08,\n",
      "         1.0603e-07, 6.3041e-08, 6.4539e-08, 7.9028e-08, 1.7975e-08, 6.1840e-09,\n",
      "         8.7737e-08, 1.1332e-07],\n",
      "        [9.9386e-07, 1.1417e-06, 1.1550e-06, 9.0610e-08, 8.1359e-07, 4.4776e-07,\n",
      "         1.0405e-06, 2.5105e-07, 8.1251e-07, 1.3083e-06, 1.2328e-06, 4.4328e-07,\n",
      "         1.1663e-06, 6.9345e-07, 7.0993e-07, 8.6930e-07, 1.9772e-07, 6.8024e-08,\n",
      "         9.6511e-07, 1.2465e-06],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.0351e-08, 1.0379e-07, 1.0500e-07, 8.2373e-09, 7.3963e-08, 4.0706e-08,\n",
      "         9.4588e-08, 2.2823e-08, 7.3865e-08, 1.1894e-07, 1.1207e-07, 4.0298e-08,\n",
      "         1.0603e-07, 6.3041e-08, 6.4539e-08, 7.9028e-08, 1.7975e-08, 6.1840e-09,\n",
      "         8.7737e-08, 1.1332e-07]]) tensor([[2.9608e-06, 5.6004e-06, 3.0012e-06, 4.7455e-06, 3.7001e-06, 2.6398e-06,\n",
      "         2.7549e-06, 3.9169e-06, 3.5126e-06, 4.4268e-06]]) tensor([[4.0026e-06, 4.2170e-06, 2.4486e-06, 4.0939e-06, 3.1673e-06, 4.2417e-06,\n",
      "         3.7490e-06, 4.2231e-06, 6.4333e-06, 5.0063e-06, 3.8784e-06, 3.0661e-06,\n",
      "         4.4139e-06, 3.9258e-06, 3.1082e-06, 3.1389e-06, 5.2064e-06, 5.2547e-06,\n",
      "         2.4517e-06, 3.7589e-06]])\n"
     ]
    }
   ],
   "source": [
    "print(W_x.grad, W_h.grad, x.grad, prev_h.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
