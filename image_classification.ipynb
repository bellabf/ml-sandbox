{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the convolutional layers is to learn important image features that can\n",
    "be used for the task at hand. Convolutional layers work by applying a filter to a\n",
    "particular area of an image (the size of the convolution). The weights of this layer\n",
    "then learn to recognize specific image features critical in the classification task. For\n",
    "instance, if we’re training a model that recognizes a person’s hand, the filter may learn\n",
    "to recognize fingers.\n",
    "The purpose of the pooling layer is typically to reduce the dimensionality of the inputs\n",
    "from the previous layer. This layer also uses a filter applied to a portion of the input,\n",
    "but it has no activation. Instead, it reduces dimensionality of the input by performing\n",
    "max pooling (where it selects the pixel in the filter with the highest value) or average\n",
    "pooling (where it takes an average of the input pixels to use instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convolutional neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(self.dropout1(x), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(self.dropout2(x)))\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to run on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define the data preprocessing steps\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "shuffle=True)\n",
    "# Initialize the model and optimizer\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# Compile the model using torch 2.0's optimizer\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "model.train()\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = nn.functional.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing loop\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # get the index of the max log-probability\n",
    "        test_loss += nn.functional.nll_loss(\n",
    "        output, target, reduction='sum'\n",
    "        ).item() # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Pretrained Model for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 01:53:54.402054: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-28 01:53:54.404584: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 01:53:54.445235: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-28 01:53:54.445266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-28 01:53:54.446391: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-28 01:53:54.453022: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 01:53:54.453638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 01:53:55.330296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torchvision.transforms import(\n",
    "RandomResizedCrop, Compose, Normalize, ToTensor\n",
    ")\n",
    "from transformers import Trainer, TrainingArguments, DefaultDataCollator\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from datasets import load_dataset, load_metric, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to convert the images into RGB\n",
    "\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in\n",
    "    examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Define a helper function to compute metrics\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1),\n",
    "    references=p.label_ids)\n",
    "\n",
    "# Load the fashion mnist dataset\n",
    "dataset = load_dataset(\"fashion_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isabella/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the processor from the VIT model\n",
    "image_processor = ViTFeatureExtractor.from_pretrained(\n",
    "\"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "# Set the labels from the dataset\n",
    "labels = dataset['train'].features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_181397/335348936.py:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/home/isabella/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the collator, normalizer, and transforms\n",
    "collate_fn = DefaultDataCollator()\n",
    "normalize = Normalize(mean=image_processor.image_mean,\n",
    "std=image_processor.image_std)\n",
    "size = (\n",
    "image_processor.size[\"shortest_edge\"]\n",
    "if \"shortest_edge\" in image_processor.size\n",
    "else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "# Load the dataset we'll use with transformations\n",
    "dataset = dataset.with_transform(transforms)\n",
    "# Use accuracy as our metric\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fashion_mnist_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a trainer\n",
    "trainer = Trainer(\n",
    "model=model,\n",
    "args=training_args,\n",
    "data_collator=collate_fn,\n",
    "compute_metrics=compute_metrics,\n",
    "train_dataset=dataset[\"train\"],\n",
    "eval_dataset=dataset[\"test\"],\n",
    "tokenizer=image_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae2bd878b7241fa8980c200c9431c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0621, 'learning_rate': 0.0010638297872340426, 'epoch': 0.01}\n",
      "{'loss': 1.8625, 'learning_rate': 0.002127659574468085, 'epoch': 0.02}\n",
      "{'loss': 2.029, 'learning_rate': 0.003191489361702128, 'epoch': 0.03}\n",
      "{'loss': 2.1504, 'learning_rate': 0.00425531914893617, 'epoch': 0.04}\n",
      "{'loss': 2.2108, 'learning_rate': 0.005319148936170213, 'epoch': 0.05}\n",
      "{'loss': 2.1254, 'learning_rate': 0.006382978723404256, 'epoch': 0.06}\n",
      "{'loss': 2.1921, 'learning_rate': 0.007446808510638297, 'epoch': 0.07}\n",
      "{'loss': 2.2257, 'learning_rate': 0.00851063829787234, 'epoch': 0.09}\n",
      "{'loss': 2.1419, 'learning_rate': 0.009574468085106383, 'epoch': 0.1}\n",
      "{'loss': 2.1191, 'learning_rate': 0.009928825622775802, 'epoch': 0.11}\n",
      "{'loss': 2.0848, 'learning_rate': 0.009810201660735469, 'epoch': 0.12}\n",
      "{'loss': 2.0774, 'learning_rate': 0.009691577698695136, 'epoch': 0.13}\n",
      "{'loss': 2.1317, 'learning_rate': 0.009572953736654805, 'epoch': 0.14}\n",
      "{'loss': 2.0978, 'learning_rate': 0.009454329774614472, 'epoch': 0.15}\n",
      "{'loss': 2.0687, 'learning_rate': 0.00933570581257414, 'epoch': 0.16}\n",
      "{'loss': 2.0744, 'learning_rate': 0.009217081850533809, 'epoch': 0.17}\n",
      "{'loss': 2.0121, 'learning_rate': 0.009098457888493476, 'epoch': 0.18}\n",
      "{'loss': 2.1346, 'learning_rate': 0.008979833926453143, 'epoch': 0.19}\n",
      "{'loss': 2.1108, 'learning_rate': 0.008861209964412812, 'epoch': 0.2}\n",
      "{'loss': 2.0482, 'learning_rate': 0.008742586002372479, 'epoch': 0.21}\n",
      "{'loss': 2.0915, 'learning_rate': 0.008623962040332148, 'epoch': 0.22}\n",
      "{'loss': 2.2007, 'learning_rate': 0.008505338078291815, 'epoch': 0.23}\n",
      "{'loss': 2.0834, 'learning_rate': 0.008386714116251482, 'epoch': 0.25}\n",
      "{'loss': 1.9965, 'learning_rate': 0.008268090154211151, 'epoch': 0.26}\n",
      "{'loss': 2.0658, 'learning_rate': 0.008149466192170818, 'epoch': 0.27}\n",
      "{'loss': 2.1535, 'learning_rate': 0.008030842230130486, 'epoch': 0.28}\n",
      "{'loss': 2.1763, 'learning_rate': 0.007912218268090155, 'epoch': 0.29}\n",
      "{'loss': 2.109, 'learning_rate': 0.007793594306049823, 'epoch': 0.3}\n",
      "{'loss': 2.0588, 'learning_rate': 0.00767497034400949, 'epoch': 0.31}\n",
      "{'loss': 2.1261, 'learning_rate': 0.007556346381969158, 'epoch': 0.32}\n",
      "{'loss': 2.1456, 'learning_rate': 0.007437722419928826, 'epoch': 0.33}\n",
      "{'loss': 2.1421, 'learning_rate': 0.007319098457888493, 'epoch': 0.34}\n",
      "{'loss': 2.0295, 'learning_rate': 0.007200474495848161, 'epoch': 0.35}\n",
      "{'loss': 2.0642, 'learning_rate': 0.00708185053380783, 'epoch': 0.36}\n",
      "{'loss': 2.1111, 'learning_rate': 0.006963226571767497, 'epoch': 0.37}\n",
      "{'loss': 2.098, 'learning_rate': 0.0068446026097271654, 'epoch': 0.38}\n",
      "{'loss': 2.1077, 'learning_rate': 0.0067259786476868335, 'epoch': 0.39}\n",
      "{'loss': 2.1672, 'learning_rate': 0.006607354685646501, 'epoch': 0.41}\n",
      "{'loss': 2.3087, 'learning_rate': 0.006488730723606169, 'epoch': 0.42}\n",
      "{'loss': 2.1648, 'learning_rate': 0.006370106761565836, 'epoch': 0.43}\n",
      "{'loss': 2.1075, 'learning_rate': 0.006251482799525504, 'epoch': 0.44}\n",
      "{'loss': 2.0913, 'learning_rate': 0.006132858837485172, 'epoch': 0.45}\n",
      "{'loss': 2.0826, 'learning_rate': 0.006014234875444839, 'epoch': 0.46}\n",
      "{'loss': 2.0767, 'learning_rate': 0.005895610913404507, 'epoch': 0.47}\n",
      "{'loss': 2.106, 'learning_rate': 0.005776986951364176, 'epoch': 0.48}\n",
      "{'loss': 1.9829, 'learning_rate': 0.0056583629893238434, 'epoch': 0.49}\n",
      "{'loss': 1.9785, 'learning_rate': 0.0055397390272835115, 'epoch': 0.5}\n",
      "{'loss': 2.0066, 'learning_rate': 0.00542111506524318, 'epoch': 0.51}\n",
      "{'loss': 2.066, 'learning_rate': 0.005302491103202847, 'epoch': 0.52}\n",
      "{'loss': 1.9785, 'learning_rate': 0.005183867141162515, 'epoch': 0.53}\n",
      "{'loss': 2.0208, 'learning_rate': 0.005065243179122183, 'epoch': 0.54}\n",
      "{'loss': 2.0182, 'learning_rate': 0.004946619217081851, 'epoch': 0.55}\n",
      "{'loss': 2.0042, 'learning_rate': 0.004827995255041518, 'epoch': 0.57}\n",
      "{'loss': 1.9896, 'learning_rate': 0.004709371293001186, 'epoch': 0.58}\n",
      "{'loss': 2.0026, 'learning_rate': 0.004590747330960854, 'epoch': 0.59}\n",
      "{'loss': 2.0531, 'learning_rate': 0.004472123368920522, 'epoch': 0.6}\n",
      "{'loss': 2.0547, 'learning_rate': 0.0043534994068801895, 'epoch': 0.61}\n",
      "{'loss': 2.086, 'learning_rate': 0.004234875444839858, 'epoch': 0.62}\n",
      "{'loss': 2.1, 'learning_rate': 0.004116251482799526, 'epoch': 0.63}\n",
      "{'loss': 2.0823, 'learning_rate': 0.003997627520759194, 'epoch': 0.64}\n",
      "{'loss': 2.027, 'learning_rate': 0.003879003558718861, 'epoch': 0.65}\n",
      "{'loss': 2.0171, 'learning_rate': 0.0037603795966785294, 'epoch': 0.66}\n",
      "{'loss': 2.0592, 'learning_rate': 0.003641755634638197, 'epoch': 0.67}\n",
      "{'loss': 2.0434, 'learning_rate': 0.0035231316725978646, 'epoch': 0.68}\n",
      "{'loss': 2.0173, 'learning_rate': 0.0034045077105575327, 'epoch': 0.69}\n",
      "{'loss': 1.9907, 'learning_rate': 0.0032858837485172003, 'epoch': 0.7}\n",
      "{'loss': 2.0071, 'learning_rate': 0.0031672597864768684, 'epoch': 0.71}\n",
      "{'loss': 2.0191, 'learning_rate': 0.0030486358244365365, 'epoch': 0.73}\n",
      "{'loss': 2.0342, 'learning_rate': 0.002930011862396204, 'epoch': 0.74}\n",
      "{'loss': 2.0172, 'learning_rate': 0.0028113879003558717, 'epoch': 0.75}\n",
      "{'loss': 2.0147, 'learning_rate': 0.00269276393831554, 'epoch': 0.76}\n",
      "{'loss': 1.9242, 'learning_rate': 0.002574139976275208, 'epoch': 0.77}\n",
      "{'loss': 2.0229, 'learning_rate': 0.0024555160142348755, 'epoch': 0.78}\n",
      "{'loss': 2.0181, 'learning_rate': 0.0023368920521945435, 'epoch': 0.79}\n",
      "{'loss': 1.9512, 'learning_rate': 0.002218268090154211, 'epoch': 0.8}\n",
      "{'loss': 1.9722, 'learning_rate': 0.002099644128113879, 'epoch': 0.81}\n",
      "{'loss': 1.9837, 'learning_rate': 0.001981020166073547, 'epoch': 0.82}\n",
      "{'loss': 1.9906, 'learning_rate': 0.001862396204033215, 'epoch': 0.83}\n",
      "{'loss': 2.0076, 'learning_rate': 0.0017437722419928825, 'epoch': 0.84}\n",
      "{'loss': 2.0686, 'learning_rate': 0.0016251482799525506, 'epoch': 0.85}\n",
      "{'loss': 1.9802, 'learning_rate': 0.0015065243179122182, 'epoch': 0.86}\n",
      "{'loss': 2.0507, 'learning_rate': 0.001387900355871886, 'epoch': 0.87}\n",
      "{'loss': 2.0325, 'learning_rate': 0.0012692763938315541, 'epoch': 0.89}\n",
      "{'loss': 1.9851, 'learning_rate': 0.001150652431791222, 'epoch': 0.9}\n",
      "{'loss': 1.9783, 'learning_rate': 0.0010320284697508896, 'epoch': 0.91}\n",
      "{'loss': 1.9769, 'learning_rate': 0.0009134045077105576, 'epoch': 0.92}\n",
      "{'loss': 1.9901, 'learning_rate': 0.0007947805456702253, 'epoch': 0.93}\n",
      "{'loss': 2.0583, 'learning_rate': 0.0006761565836298932, 'epoch': 0.94}\n",
      "{'loss': 2.0272, 'learning_rate': 0.0005575326215895611, 'epoch': 0.95}\n",
      "{'loss': 1.9924, 'learning_rate': 0.00043890865954922893, 'epoch': 0.96}\n",
      "{'loss': 1.9596, 'learning_rate': 0.00032028469750889683, 'epoch': 0.97}\n",
      "{'loss': 1.9871, 'learning_rate': 0.00020166073546856468, 'epoch': 0.98}\n",
      "{'loss': 1.9519, 'learning_rate': 8.303677342823249e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd25fb7518740f6a12c96cddf0b564c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the model, log and save metrics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      3\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n\u001b[1;32m      4\u001b[0m trainer\u001b[39m.\u001b[39mlog_metrics(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, train_results\u001b[39m.\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/transformers/trainer.py:1944\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1941\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1944\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1946\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1947\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1948\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/transformers/trainer.py:2256\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2254\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2259\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/transformers/trainer.py:2972\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2969\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2971\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2972\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2973\u001b[0m     eval_dataloader,\n\u001b[1;32m   2974\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2975\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2976\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2977\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2978\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2979\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2980\u001b[0m )\n\u001b[1;32m   2982\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-zoomcamp/lib/python3.9/site-packages/transformers/trainer.py:3261\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3257\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3258\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3259\u001b[0m         )\n\u001b[1;32m   3260\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3261\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(p):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39margmax(p\u001b[39m.\u001b[39mpredictions, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m     references\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mlabel_ids)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model, log and save metrics\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the realm of unstructured data like text and images, it is extremely common\n",
    "to start from pretrained models trained on large datasets, instead of starting from\n",
    "scratch, especially in cases where we don’t have access to as much labeled data. Using\n",
    "embeddings and other information from the larger model, we can then fine-tune\n",
    "our own model for a new task without the need for as much labeled information.\n",
    "In addition, the pretrained model may have information not captured at all in our\n",
    "training dataset, resulting in an overall performance improvement. This process is\n",
    "known as transfer learning.\n",
    "In this example, we load the weights from Google’s ViT (Vision Transformer) model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
